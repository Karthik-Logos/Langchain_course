{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangChain Basics\n",
    "\n",
    "In this notebook, we will dive deep into the basics of LangChain, covering essential components and their usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Prompt:**\n",
    "A prompt is the input given to a Large Language Model (LLM) to generate a response. It acts as an instruction or question that guides the model to produce meaningful and relevant output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Use Prompt Templates Instead of Direct Prompts?**\n",
    "- A Prompt Template is a structured way to format prompts dynamically, making them reusable and more efficient. Instead of hardcoding prompts, Prompt Templates allow parameterized inputs."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "- Instead of writing a new prompt every time, templates allow us to use the same structure with different inputs.\n",
    "- The same template can be used for \"Germany\", \"India\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates are a powerful feature in LangChain that allow you to define how inputs are structured when sent to a language model. They can be used for various prompting strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Prompt Templates\n",
    "\n",
    "Prompt templates help structure the input for LLMs. There are different prompting techniques:\n",
    "- Zero-shot prompting → No examples, just a direct request.\n",
    "- Few-shot prompting → Provides examples to guide the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Simple LLM Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key in the environment variable\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Quantum computing is a type of computing that uses the principles of quantum mechanics to process information differently from traditional (classical) computing.\n",
      "\n",
      "Here are the key points to understand quantum computing in simple terms:\n",
      "\n",
      "1. **Quantum Bits (Qubits)**:\n",
      "   - In classical computing, the basic unit of information is the bit, which can be either 0 or 1. However, in quantum computing, the basic unit of information is the quantum bit, or qubit. Unlike a bit, a qubit can be in a state of 0, 1, or any quantum superposition of these states. This means a qubit can perform multiple calculations at once.\n",
      "\n",
      "2. **Superposition**:\n",
      "   - This is a fundamental principle of quantum mechanics where a quantum system can be in multiple states at the same time. For qubits, it means they can be 0 and 1 simultaneously, which allows quantum computers to process a vast amount of possibilities simultaneously.\n",
      "\n",
      "3. **Entanglement**:\n",
      "   - This is another quantum phenomenon where qubits become interconnected and the state of one (whether it's 0 or 1) can depend on the state of another. This helps quantum computers to perform complex calculations more quickly because the state of one qubit can instantly affect others, no matter the distance between them.\n",
      "\n",
      "4. **Interference**:\n",
      "   - Quantum computers use interference to amplify correct paths that lead to answers while canceling out paths that lead to wrong answers. This helps in arriving at the final answer swiftly and efficiently.\n",
      "\n",
      "5. **Speed and Complexity**:\n",
      "   - Thanks to these properties, quantum computers can theoretically solve certain problems much faster than classical computers. Problems that involve a huge amount of data and require complex computation, like factoring large numbers, simulating molecules for drug discovery, or optimizing large systems, can potentially be solved quicker by quantum computers.\n",
      "\n",
      "In summary, quantum computing is an exciting field that leverages the strange and complex rules of quantum mechanics to process information in new, powerful ways that are different from traditional computing, offering potential breakthroughs in various scientific and engineering fields.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a simple prompt template with placeholders\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Can you explain {topic} in simple terms?\")\n",
    "])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "# Format the prompt with a specific topic\n",
    "formatted_prompt = prompt.format_messages(topic=\"Quantum Computing\")\n",
    "\n",
    "# Get the response from the LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Zero-shot Prompting (No examples)\n",
    "- Zero-shot prompting is a technique where an AI model is asked to perform a task without being given any prior examples or training on that specific task. The model relies entirely on its pre-trained knowledge to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python and Java are both powerful, widely-used programming languages, but they have distinct characteristics that make them suitable for different types of projects. Here are some key differences between Python and Java:\n",
      "\n",
      "1. **Syntax and Code Readability:**\n",
      "   - **Python:** Known for its simple and readable syntax, Python emphasizes code readability and simplicity. It avoids using curly braces and semicolons, making use of indentation to define code blocks.\n",
      "   - **Java:** Compared to Python, Java has a more verbose and complex syntax. It uses explicit declarations and has strict rules for defining classes and methods, which includes mandatory semicolons and curly braces.\n",
      "\n",
      "2. **Typing System:**\n",
      "   - **Python:** Python is dynamically typed, meaning that the type of a variable is determined at runtime and you do not need to explicitly declare variable types. This can lead to greater flexibility but increases the possibility of runtime errors.\n",
      "   - **Java:** Java is statically typed, requiring explicit declarations of variable types. This extra detail helps prevent certain types of errors and enhances code clarity at the expense of additional verbosity.\n",
      "\n",
      "3. **Performance:**\n",
      "   - **Python:** Generally, Python programs tend to run slower than Java programs. This is because Python is an interpreted language, and the inherent overhead of dynamic typing can slow execution.\n",
      "   - **Java:** Java applications tend to run faster than Python applications. Java benefits from the Just-In-Time (JIT) compiler of the Java Virtual Machine (JVM), which allows it to optimize code execution based on the execution environment and improve performance over time.\n",
      "\n",
      "4. **Memory Management:**\n",
      "   - **Python:** Uses automatic memory management and garbage collection, handled by Python’s memory manager. This simplifies the work for developers but reduces control over memory allocation.\n",
      "   - **Java:** Also uses garbage collection, but it provides more visibility and control over memory utilization, which is advantageous in scenarios where memory is a constraint.\n",
      "\n",
      "5. **Concurrency:**\n",
      "   - **Python:** Multi-threading in Python is limited by the Global Interpreter Lock (GIL), which allows only one thread to execute Python code at a time in a single process. This can be a limiting factor for CPU-bound applications that require true parallelism.\n",
      "   - **Java:** Provides powerful options for multithreading without the GIL constraint, making it better suited for applications with high levels of concurrency.\n",
      "\n",
      "6. **Ecosystem and Libraries:**\n",
      "   - **Python:** Has a rich ecosystem particularly strong in areas such as data science, machine learning, artificial intelligence, web development (Django, Flask), and scripting. The extensive availability of libraries and frameworks simplifies the development of such applications.\n",
      "   - **Java:** Also boasts a robust ecosystem with a strong presence in enterprise environments, Android app development, and large systems with high scalability requirements. Libraries and frameworks like Spring and Hibernate have established Java as a first choice in many traditional enterprise settings.\n",
      "\n",
      "7. **Community and Popularity:**\n",
      "   - Both languages have large, active communities and are widely popular. Java has been a standard choice in large enterprises for many years, whereas Python has grown significantly in popularity and is preferred in academia and data-oriented industries.\n",
      "\n",
      "8. **Learning Curve:**\n",
      "   - **Python:** Generally considered easier for beginners due to its readability and simplicity.\n",
      "   - **Java:** May take longer to master because of its verbose syntax and complex rules.\n",
      "\n",
      "The choice between Python and Java largely depends on the specific requirements of a project and the environment in which the software will operate. Python is a great choice for prototyping and rapid application development, while Java is suited for developing high-performance, scalable applications for enterprises.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")  # Adjust model as needed\n",
    "\n",
    "# Define the zero-shot prompt\n",
    "prompt = \"What are the key differences between Python and Java?\"\n",
    "\n",
    "# Get response\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "# Print output\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Few-shot Prompting (With examples)\n",
    "- Few-shot prompting is a technique where you provide a few examples in the prompt to help the model generate more accurate and contextually relevant responses.\n",
    "\n",
    "- Unlike Zero-shot prompting, which relies entirely on the model's pre-trained knowledge, few-shot prompting guides the model with examples, improving its performance on specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a field of AI that gives computers the ability to understand, interpret, and generate human language.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the few-shot prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI assistant providing concise explanations.\"),\n",
    "    \n",
    "    # Few-shot examples\n",
    "    (\"human\", \"What is Machine Learning?\"),\n",
    "    (\"ai\", \"Machine Learning is a subset of AI that enables computers to learn from data without being explicitly programmed.\"),\n",
    "    \n",
    "    (\"human\", \"What is Deep Learning?\"),\n",
    "    (\"ai\", \"Deep Learning is a subset of Machine Learning that uses neural networks with multiple layers to process complex data.\"),\n",
    "    \n",
    "    # User query\n",
    "    (\"human\", \"What is Natural Language Processing?\")\n",
    "])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format_messages()\n",
    "\n",
    "# Get the response from the LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an LLM in LangChain\n",
    "\n",
    "- In LangChain, \"LLM\" stands for Large Language Model, which is the core AI component that powers the framework. LangChain provides abstractions to work with various language models such as OpenAI's GPT models, Anthropic's Claude, Google's models, and open-source models like Llama.\n",
    "\n",
    "- LangChain's LLM component serves as a standardized interface that allows you to interact with any supported language model using consistent methods and parameters, regardless of which provider or model you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Wrappers\n",
    "\n",
    "LangChain provides wrappers for various language models, making it easier to switch between them without changing your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of LLM Wrappers\n",
    "- 1️⃣ LangChain – A powerful framework for building applications with LLMs.\n",
    "- 2️⃣ LlamaIndex (GPT Index) – Helps in structuring and retrieving documents for LLMs.\n",
    "- 3️⃣ Transformers (Hugging Face) – A library for interacting with various open-source models.\n",
    "- 4️⃣ GPT-Index – Optimized for indexing and searching large documents.\n",
    "- 5️⃣ OpenAI Python SDK – A direct wrapper around OpenAI’s API for easy integration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported LLM Wrappers by LangChain\n",
    "\n",
    "1. **OpenAI**\n",
    "   - Popular for its GPT models\n",
    "2. **Hugging Face**\n",
    "   - Access to a wide range of models\n",
    "3. **Claude**\n",
    "   - Known for conversational AI\n",
    "4. **Gemini**\n",
    "   - Specialized in domain-specific models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "res =llm(\"Tell me a joke about programming.\")\n",
    "res.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_huggingface_api_token\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Translate 'Hello' to French.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key\"\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "response = llm.invoke(\"Summarize the history of AI.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "A chain in LangChain is a sequence of LLM calls or other components (like prompts, memory, or tools) connected together to achieve a task. Instead of calling a single LLM directly, chains allow you to combine multiple steps into a structured workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Chains\n",
    "\n",
    "1. **Sequential Chains**\n",
    "   - Execute steps in a predefined order\n",
    "2. **Router Chains**\n",
    "   - Direct inputs to different chains based on conditions\n",
    "3. **Custom Chains**\n",
    "   - Create your own logic for chaining operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Sequential Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El sol es una estrella que emite calor y luz.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the OpenAI LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# First chain: Summarization\n",
    "summarization_prompt = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{text}\")\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "\n",
    "# Second chain: Translation to Spanish\n",
    "translation_prompt = PromptTemplate.from_template(\"Translate the following English text to Spanish:\\n\\n{text}\")\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "\n",
    "# Create a sequential chain\n",
    "chain = SimpleSequentialChain(chains=[summarization_chain, translation_chain])\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(\"The sun is a star that provides heat and light.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Router Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is a fundamental force of nature that causes objects with mass to be attracted to each other. It is responsible for the phenomenon of weight, as well as the motion of celestial bodies such as planets, stars, and galaxies. According to the theory of general relativity proposed by Albert Einstein, gravity is the result of the curvature of spacetime caused by mass and energy. This curvature causes objects to follow curved paths in spacetime, which we perceive as the force of gravity pulling objects towards each other. Gravity is a weak force compared to other fundamental forces such as electromagnetism, but it has a significant impact on the structure and behavior of the universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define prompts\n",
    "science_prompt = PromptTemplate.from_template(\"Explain {topic} in a scientific way.\")\n",
    "casual_prompt = PromptTemplate.from_template(\"Explain {topic} in a fun, simple way.\")\n",
    "\n",
    "# Create chains\n",
    "science_chain = science_prompt | llm\n",
    "casual_chain = casual_prompt | llm\n",
    "\n",
    "# Define router logic\n",
    "router_chain = RunnableBranch(\n",
    "    (lambda x: x[\"chain_type\"] == \"science\", science_chain),\n",
    "    (lambda x: x[\"chain_type\"] == \"casual\", casual_chain),\n",
    "    casual_chain  # Default fallback\n",
    ")\n",
    "\n",
    "# Run the router chain\n",
    "response = router_chain.invoke({\"topic\": \"gravity\", \"chain_type\": \"science\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Custom Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instructions': \"Cooking biryani is a delightful process that involves layering marinated meat with partially cooked rice and then cooking it together to allow the flavors to meld. Here's a basic recipe for chicken biryani, but you can adapt it for other proteins or make it vegetarian:\", 'summary': '### Ingredients:'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key='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')\n",
    "\n",
    "# Define prompt template\n",
    "prompt = PromptTemplate.from_template(\"How do you cook {food}?\")\n",
    "\n",
    "# Custom output parser to structure the response\n",
    "class CookingOutputParser(StrOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        # Split the response into instructions and summary\n",
    "        parts = text.split(\"\\n\\n\")\n",
    "        instructions = parts[0].strip() if len(parts) > 0 else \"\"\n",
    "        summary = parts[1].strip() if len(parts) > 1 else \"No summary available.\"\n",
    "        return {\n",
    "            \"instructions\": instructions,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "\n",
    "# Create a chain (Prompt -> LLM -> Output Parser)\n",
    "chain = (\n",
    "    prompt \n",
    "    | llm \n",
    "    | CookingOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"food\": \"Biriyani\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LLMChain (Basic Single-Step Chain)\n",
    "The simplest chain that combines a prompt template with an LLM.\n",
    "📌 Use case: Sending a prompt and getting a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_21756\\3389187055.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_21756\\3389187055.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_21756\\3389187055.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(\"LangChain\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a platform that uses blockchain technology to provide language translation services. It aims to connect users with professional translators in a secure and efficient manner, ensuring accurate and high-quality translations. LangChain also offers features such as real-time collaboration, secure payments, and transparent pricing to make the translation process easier for both clients and translators.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt = PromptTemplate.from_template(\"What is {topic}?\")\n",
    "\n",
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(\"LangChain\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Memory in LangChain allows applications to store and recall past interactions, making conversations more context-aware. Without memory, each interaction is independent, meaning the AI won’t remember previous inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Use Memory?**\n",
    "- Maintains conversation context over multiple exchanges.\n",
    "- Improves user experience by allowing follow-up questions.\n",
    "- Reduces token usage by summarizing past conversations instead of re-sending entire histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **ConversationBufferMemory**\n",
    "- Keeps the entire conversation history.\n",
    "- Best for short, casual conversations.\n",
    "- Becomes inefficient for long conversations due to increasing token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_17720\\3036349091.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_17720\\3036349091.py:12: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Alex! It's nice to meet you. How can I assist you today?\n",
      "Your name is Alex.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0 , api_key='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')\n",
    "\n",
    "# Initialize memory for conversation\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Start conversation\n",
    "print(conversation.invoke(\"Hi, I am Alex.\")[\"response\"])\n",
    "print(conversation.invoke(\"What is my name?\")[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ConversationBufferWindowMemory (Stores last ‘N’ messages)\n",
    "- Retains only the last few interactions (fixed window size).\n",
    "- Good for keeping recent context while saving tokens.\n",
    "- Useful for long conversations where full history isn’t needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_15760\\1868798637.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2)  # Keeps last 3 exchanges\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_15760\\1868798637.py:9: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_15760\\1868798637.py:13: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Why couldn't the bicycle find its way home? Because it lost its bearings!\n",
      "Of course! In this joke, \"lost its bearings\" is a play on words. Normally, when someone says they \"lost their bearings,\" it means they are lost and can't find their way. In the case of the bicycle, it can't find its way home because it literally lost its bearings, which are the parts that allow the wheels to spin smoothly. So, it's a pun that combines the literal and figurative meanings of the phrase \"lost its bearings.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)  # Keeps last 3 exchanges\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.predict(input=\"Tell me a joke.\"))\n",
    "print(conversation.predict(input=\"Can you explain that joke?\"))  # AI remembers the joke!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ConversationSummaryMemory (Summarizes past conversations)\n",
    "- Condenses conversations into summaries.\n",
    "- Reduces token usage while maintaining context.\n",
    "- Useful for long-running conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_15760\\2349614694.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why couldn't the bicycle stand up by itself? Because it was two-tired!\n",
      "Of course! The joke is a play on words. The word \"tired\" sounds like \"tired,\" which is how you would describe someone who needs rest. In the joke, the bicycle couldn't stand up by itself because it was \"two-tired,\" meaning it had two tires (wheels) and also needed rest. It's a pun that combines the concept of being tired with the number of tires on a bicycle.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.predict(input=\"Tell me a joke.\"))\n",
    "print(conversation.predict(input=\"Can you explain that joke?\"))  # AI remembers the joke!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ConversationKGMemory (Stores knowledge graph)\n",
    "- Stores key facts and relationships from conversations.\n",
    "- Good for structured data extraction.\n",
    "- Useful for personal assistants and FAQ bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? Because they make up everything!\n",
      "Sure! The joke is a play on words that relies on a pun or a double meaning. It's meant to be funny because of the unexpected twist or clever wordplay. Would you like me to explain a specific joke?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.predict(input=\"Tell me a joke.\"))\n",
    "print(conversation.predict(input=\"Can you explain that joke?\"))  # AI remembers the joke!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the basics of LangChain, covering prompt templates, LLM wrappers, chains, and memory. Each component plays a crucial role in building effective and sophisticated AI applications.\n",
    "\n",
    "In the next notebook, we will delve into more advanced topics and explore real-world applications of LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

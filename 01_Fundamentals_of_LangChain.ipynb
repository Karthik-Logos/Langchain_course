{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals of LangChain\n",
    "\n",
    "## Overview of LangChain\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications powered by Language Models (LLMs). Think of it as a toolkit that helps you build sophisticated AI applications by connecting various components together.\n",
    "\n",
    "### Why Use LangChain?\n",
    "- **Simplifies LLM Integration**: Makes it easier to work with language models\n",
    "- **Promotes Reusability**: Provides ready-to-use components\n",
    "- **Enhances Flexibility**: Supports multiple LLM providers (OpenAI, Anthropic, etc.)\n",
    "- **Standardizes Development**: Offers consistent patterns for building applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangChain Image](https://daxg39y63pxwu.cloudfront.net/images/blog/langchain/LangChain.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Components of LangChain**\n",
    "\n",
    "### 1. Language Model (LLM)\n",
    "- The core component that powers text generation.\n",
    "- Supports various LLM providers like OpenAI, Anthropic, and Google.\n",
    "\n",
    "### 2. Prompt Templates\n",
    "- Helps structure and format prompts dynamically.\n",
    "- Useful for standardizing inputs to the LLM.\n",
    "\n",
    "### 3. Chains\n",
    "- Sequences multiple steps together, such as retrieving data and generating responses.\n",
    "- Example: Input â†’ Retrieval â†’ LLM â†’ Output.\n",
    "\n",
    "### 4. Memory\n",
    "- Enables storing and recalling conversation history.\n",
    "- Useful for chatbots and contextual interactions.\n",
    "\n",
    "### 5. Agents\n",
    "- AI components that make decisions dynamically.\n",
    "- Can decide which tools to call based on the input.\n",
    "\n",
    "### 6. Tools\n",
    "- External functionalities an agent can use, such as API calls, web searches, or calculations.\n",
    "- Extends the modelâ€™s capabilities beyond text generation.\n",
    "\n",
    "### 7. Retrieval & Vector Stores\n",
    "- Helps fetch relevant data using embeddings.\n",
    "- Supports databases like FAISS, Pinecone, and Chroma.\n",
    "\n",
    "### 8. Document Loaders\n",
    "- Reads and processes files (PDFs, CSVs, text, etc.).\n",
    "- Useful for knowledge retrieval applications.\n",
    "\n",
    "### 9. Output Parsers\n",
    "- Structures the output into a machine-readable format.\n",
    "- Converts responses into JSON, tables, or structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of LangChain\n",
    "\n",
    "### 1. Component-Based Architecture\n",
    "- **Modular Design**: Components can be mixed, matched, and customized\n",
    "- **Composability**: Build complex applications by combining simple components\n",
    "- **Extensibility**: Easy to create custom components to meet specific needs\n",
    "\n",
    "### 2. LLM Abstraction and Integration\n",
    "- **Model Agnostic**: Works with multiple LLM providers (OpenAI, Anthropic, local models, etc.)\n",
    "- **Simple Switching**: Easily switch between different models without changing application logic\n",
    "- **Parameter Standardization**: Consistent interface across different model providers\n",
    "\n",
    "### 3. Advanced Chains\n",
    "- **Sequential Processing**: Chain multiple steps together in logical sequences\n",
    "- **Conditional Logic**: Implement branching and decision-making in workflows\n",
    "- **Specialized Chain Types**: SequentialChain, RouterChain, TransformChain, and more\n",
    "\n",
    "### 4. Memory Systems\n",
    "- **Conversation Buffers**: Store and retrieve conversation history\n",
    "- **Summary Memory**: Maintain summaries of longer conversations\n",
    "- **Vector-Based Memory**: Store information based on semantic relevance\n",
    "- **Multiple Memory Types**: ConversationBufferMemory, ConversationSummaryMemory, VectorStoreMemory\n",
    "\n",
    "### 5. Agent Frameworks\n",
    "- **Autonomous Decision-Making**: Agents that can plan and execute multi-step tasks\n",
    "- **Tool Integration**: Enables LLMs to use external tools and functions\n",
    "- **ReAct Framework**: Reasoning and acting based on environment feedback\n",
    "- **Agent Types**: Zero-shot, Plan-and-execute, OpenAI functions agents, etc.\n",
    "\n",
    "### 6. Document Processing\n",
    "- **Multiple Loaders**: Import data from diverse sources (PDFs, websites, databases)\n",
    "- **Text Splitters**: Divide documents into chunks for processing\n",
    "- **Document Transformers**: Process and enhance document content\n",
    "\n",
    "### 7. Vectorstores and Retrieval\n",
    "- **Vector Databases**: Store and query data based on semantic similarity\n",
    "- **Embedding Integration**: Works with multiple embedding models\n",
    "- **Retrieval Types**: Similarity search, MMR, filtering\n",
    "- **Integration**: Connects with many vector databases (Pinecone, Chroma, FAISS, etc.)\n",
    "\n",
    "### 8. Prompt Management\n",
    "- **Templating**: Create and reuse prompt templates\n",
    "- **Dynamic Generation**: Dynamically construct prompts based on context\n",
    "- **Optimization Tools**: Improve prompts for better results\n",
    "\n",
    "### 9. Evaluation and Debugging\n",
    "- **Tracing Framework**: Track execution of chains and agents\n",
    "- **Callbacks System**: Hook into the execution process\n",
    "- **Metrics Collection**: Evaluate performance of components\n",
    "\n",
    "### 10. Streaming Support\n",
    "- **Token Streaming**: Process outputs as they're generated\n",
    "- **Websocket Integration**: Real-time communication in web applications\n",
    "- **Incremental Processing**: Handle partial results effectively\n",
    "\n",
    "These features make LangChain particularly powerful for building sophisticated AI applications that go beyond simple prompt-response interactions, enabling complex workflows that combine language models with external data and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simple LLM call using LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: LangChain is a blockchain-based platform that aims to create a decentralized marketplace for language services. It allows users to connect with language service providers, such as translators, interpreters, and language teachers, directly through smart contracts. The platform also enables secure payments and provides a reputation system to ensure quality service. LangChain aims to disrupt the traditional language services industry by eliminating intermediaries and streamlining the process of finding and hiring language professionals.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the LLM (Make sure to set OPENAI_API_KEY in your environment)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7 , api_key=\"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\")\n",
    "\n",
    "# Example of a simple LLM call\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(\"LLM Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for Langchain components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't worry if you don't understand this right now. We'll explore all of this in detail in later notebooks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI can be categorized into two types:\n",
      "\n",
      "1. **Narrow AI (Weak AI):** This type of AI is designed and trained for a specific task, such as voice assistants like Siri or Alexa, recommendation systems, or autonomous vehicles. It operates under a limited set of constraints and is not capable of generalizing its knowledge to other domains.\n",
      "\n",
      "2. **General AI (Strong AI):** This is a theoretical form of AI that possesses the ability to perform any intellectual task that a human can do. It would have the ability to understand, learn, and apply knowledge in a generalized way, but it does not yet exist.\n",
      "\n",
      "AI technologies include machine learning, natural language processing, robotics, and computer vision, among others. These technologies enable machines to perform tasks that typically require human intelligence, such as recognizing speech, making decisions, and translating languages.\n",
      "QA Response: {'answer': 'Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It is used to automate processes, improve decision-making, and enhance customer experiences across various industries.', 'confidence': 0.95}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Updated OpenAI imports\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers.structured import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "\n",
    "# 1. Language Model (LLM)\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# 2. Prompt Templates\n",
    "# Agent prompt template (requires agent_scratchpad)\n",
    "agent_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # For memory\n",
    "    (\"user\", \"{input}\"),  # User input\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")  # Required for OpenAI tools agent\n",
    "])\n",
    "\n",
    "# QA prompt template (doesn't need agent_scratchpad)\n",
    "qa_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # For memory\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "    (\"user\", \"Question: {input}\\nContext: {context}\")  # User input with context\n",
    "])\n",
    "\n",
    "# 3. Memory\n",
    "# For the agent - uses default input_key=\"input\"\n",
    "agent_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# For the QA chain - specify the input_key explicitly\n",
    "qa_memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\", return_messages=True)\n",
    "\n",
    "# 4. Agents and Tools\n",
    "def custom_tool_function(input_text):\n",
    "    return f\"You provided: {input_text}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"CustomTool\",\n",
    "        func=custom_tool_function,\n",
    "        description=\"A tool that echoes back the input text.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the OpenAI tools agent\n",
    "agent_executor = create_openai_tools_agent(llm, tools, agent_prompt_template)\n",
    "agent = AgentExecutor(agent=agent_executor, tools=tools, memory=agent_memory)\n",
    "\n",
    "# 5. Document Loaders\n",
    "loader = TextLoader(\"sample.txt\")  # Create a file named \"sample.txt\" with some text content.\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 6. Retrieval & Vector Stores\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 7. Create a QA chain with the correct prompt template\n",
    "qa_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=qa_prompt_template,  # Using the QA-specific prompt template\n",
    "    memory=qa_memory\n",
    ")\n",
    "\n",
    "# 8. Output Parsers\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", type=\"string\", description=\"The answer to the question\"),\n",
    "    ResponseSchema(name=\"confidence\", type=\"float\", description=\"Confidence score between 0 and 1\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Combine everything\n",
    "def process_query(question):\n",
    "    # Use the agent for dynamic decision-making\n",
    "    agent_response = agent.invoke({\"input\": question})\n",
    "    \n",
    "    # Retrieve relevant context from the vector store\n",
    "    relevant_context = vectorstore.similarity_search(question, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_context])\n",
    "    \n",
    "    # Use the QA chain for knowledge-based responses with instructions\n",
    "    formatted_question = f\"{question}\\n\\n{format_instructions}\"\n",
    "    qa_response = qa_chain.invoke({\n",
    "        \"input\": formatted_question, \n",
    "        \"context\": context\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Parse the output into structured data\n",
    "        parsed_output = output_parser.parse(qa_response[\"text\"])\n",
    "    except Exception as e:\n",
    "        # Handle parsing errors gracefully\n",
    "        parsed_output = {\"answer\": qa_response[\"text\"], \"confidence\": 0.5}\n",
    "        print(f\"Warning: Could not parse response - {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        \"agent_response\": agent_response[\"output\"],\n",
    "        \"qa_response\": parsed_output\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is AI\"\n",
    "    result = process_query(question)\n",
    "    print(\"Agent Response:\", result[\"agent_response\"])\n",
    "    print(\"QA Response:\", result[\"qa_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an AI-powered question-answering system using LangChain and OpenAI. Here's what it does step by step:\n",
    "\n",
    "1. **Setup**: It imports necessary libraries and sets up an OpenAI API key to access AI models.\n",
    "\n",
    "2. **Two AI Components**:\n",
    "   - An \"Agent\" that can use tools and make decisions\n",
    "   - A \"QA Chain\" that answers questions using relevant information\n",
    "\n",
    "3. **Memory System**: Both components remember previous conversations so they can maintain context.\n",
    "\n",
    "4. **Document Processing**:\n",
    "   - The system loads text from a file called \"sample.txt\"\n",
    "   - It breaks this text into smaller chunks\n",
    "   - It creates a searchable database of these chunks using embeddings (vector representations of text)\n",
    "\n",
    "5. **Question Processing Flow**:\n",
    "   - When you ask a question, the Agent tries to answer it using its tools\n",
    "   - The system also searches for relevant information in the document database\n",
    "   - The QA Chain uses this relevant information to provide a more informed answer\n",
    "   - The system tries to structure the answer with a confidence score\n",
    "\n",
    "6. **Output Format**: For each question, you get:\n",
    "   - An \"agent response\" (direct from the AI)\n",
    "   - A \"QA response\" (based on the document knowledge with structured format)\n",
    "\n",
    "The main advantage of this system is that it combines general AI capabilities with specific knowledge from your documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't worry if you don't understand this right now. We'll explore all of this in detail in later notebooks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Setup**\n",
    "\n",
    "Let's get started with installing LangChain and its dependencies. We'll go through this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Python (if not installed)\n",
    "Download and install Python (version 3.8 or later) from:\n",
    "ðŸ”— https://www.python.org/downloads/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a Virtual Environment (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### On Windows (Command Prompt or PowerShell)\n",
    "- python -m venv langchain_env\n",
    "- langchain_env\\Scripts\\activate\n",
    "\n",
    "###### On macOS/Linux (Terminal)\n",
    "- python -m venv langchain_env\n",
    "- source langchain_env/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set Up OpenAI API Key\n",
    "You need an API key from OpenAI:\n",
    "\n",
    "- Sign up at ðŸ”— https://platform.openai.com/signup\n",
    "- Get your API key from ðŸ”— https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On Windows (Command Prompt)\n",
    "set OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "##### On PowerShell\n",
    "$env:OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run a Simple Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: One fun fact about space is that the International Space Station travels at a speed of approximately 17,500 miles per hour, orbiting the Earth once every 90 minutes. This means that astronauts on board the ISS experience 16 sunrises and sunsets every day!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Test call\n",
    "response = llm.invoke(\"Tell me a fun fact about space.\")\n",
    "\n",
    "print(\"LLM Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Now that we have covered the fundamentals and set up our environment, in the next notebooks we'll explore:\n",
    "1. Working with different types of LLMs\n",
    "2. Creating and using Chains\n",
    "3. Understanding Prompts and Templates\n",
    "4. Implementing Memory in our applications\n",
    "5. Working with Agents and Tools\n",
    "\n",
    "Each topic will include practical examples and real-world use cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals of LangChain\n",
    "\n",
    "## Overview of LangChain\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications powered by Language Models (LLMs). Think of it as a toolkit that helps you build sophisticated AI applications by connecting various components together.\n",
    "\n",
    "### Why Use LangChain?\n",
    "- **Simplifies LLM Integration**: Makes it easier to work with language models\n",
    "- **Promotes Reusability**: Provides ready-to-use components\n",
    "- **Enhances Flexibility**: Supports multiple LLM providers (OpenAI, Anthropic, etc.)\n",
    "- **Standardizes Development**: Offers consistent patterns for building applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangChain Image](https://daxg39y63pxwu.cloudfront.net/images/blog/langchain/LangChain.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Components of LangChain**\n",
    "\n",
    "### 1. Language Model (LLM)\n",
    "- The core component that powers text generation.\n",
    "- Supports various LLM providers like OpenAI, Anthropic, and Google.\n",
    "\n",
    "### 2. Prompt Templates\n",
    "- Helps structure and format prompts dynamically.\n",
    "- Useful for standardizing inputs to the LLM.\n",
    "\n",
    "### 3. Chains\n",
    "- Sequences multiple steps together, such as retrieving data and generating responses.\n",
    "- Example: Input → Retrieval → LLM → Output.\n",
    "\n",
    "### 4. Memory\n",
    "- Enables storing and recalling conversation history.\n",
    "- Useful for chatbots and contextual interactions.\n",
    "\n",
    "### 5. Agents\n",
    "- AI components that make decisions dynamically.\n",
    "- Can decide which tools to call based on the input.\n",
    "\n",
    "### 6. Tools\n",
    "- External functionalities an agent can use, such as API calls, web searches, or calculations.\n",
    "- Extends the model’s capabilities beyond text generation.\n",
    "\n",
    "### 7. Retrieval & Vector Stores\n",
    "- Helps fetch relevant data using embeddings.\n",
    "- Supports databases like FAISS, Pinecone, and Chroma.\n",
    "\n",
    "### 8. Document Loaders\n",
    "- Reads and processes files (PDFs, CSVs, text, etc.).\n",
    "- Useful for knowledge retrieval applications.\n",
    "\n",
    "### 9. Output Parsers\n",
    "- Structures the output into a machine-readable format.\n",
    "- Converts responses into JSON, tables, or structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of LangChain\n",
    "\n",
    "### 1. Component-Based Architecture\n",
    "- **Modular Design**: Components can be mixed, matched, and customized\n",
    "- **Composability**: Build complex applications by combining simple components\n",
    "- **Extensibility**: Easy to create custom components to meet specific needs\n",
    "\n",
    "### 2. LLM Abstraction and Integration\n",
    "- **Model Agnostic**: Works with multiple LLM providers (OpenAI, Anthropic, local models, etc.)\n",
    "- **Simple Switching**: Easily switch between different models without changing application logic\n",
    "- **Parameter Standardization**: Consistent interface across different model providers\n",
    "\n",
    "### 3. Advanced Chains\n",
    "- **Sequential Processing**: Chain multiple steps together in logical sequences\n",
    "- **Conditional Logic**: Implement branching and decision-making in workflows\n",
    "- **Specialized Chain Types**: SequentialChain, RouterChain, TransformChain, and more\n",
    "\n",
    "### 4. Memory Systems\n",
    "- **Conversation Buffers**: Store and retrieve conversation history\n",
    "- **Summary Memory**: Maintain summaries of longer conversations\n",
    "- **Vector-Based Memory**: Store information based on semantic relevance\n",
    "- **Multiple Memory Types**: ConversationBufferMemory, ConversationSummaryMemory, VectorStoreMemory\n",
    "\n",
    "### 5. Agent Frameworks\n",
    "- **Autonomous Decision-Making**: Agents that can plan and execute multi-step tasks\n",
    "- **Tool Integration**: Enables LLMs to use external tools and functions\n",
    "- **ReAct Framework**: Reasoning and acting based on environment feedback\n",
    "- **Agent Types**: Zero-shot, Plan-and-execute, OpenAI functions agents, etc.\n",
    "\n",
    "### 6. Document Processing\n",
    "- **Multiple Loaders**: Import data from diverse sources (PDFs, websites, databases)\n",
    "- **Text Splitters**: Divide documents into chunks for processing\n",
    "- **Document Transformers**: Process and enhance document content\n",
    "\n",
    "### 7. Vectorstores and Retrieval\n",
    "- **Vector Databases**: Store and query data based on semantic similarity\n",
    "- **Embedding Integration**: Works with multiple embedding models\n",
    "- **Retrieval Types**: Similarity search, MMR, filtering\n",
    "- **Integration**: Connects with many vector databases (Pinecone, Chroma, FAISS, etc.)\n",
    "\n",
    "### 8. Prompt Management\n",
    "- **Templating**: Create and reuse prompt templates\n",
    "- **Dynamic Generation**: Dynamically construct prompts based on context\n",
    "- **Optimization Tools**: Improve prompts for better results\n",
    "\n",
    "### 9. Evaluation and Debugging\n",
    "- **Tracing Framework**: Track execution of chains and agents\n",
    "- **Callbacks System**: Hook into the execution process\n",
    "- **Metrics Collection**: Evaluate performance of components\n",
    "\n",
    "### 10. Streaming Support\n",
    "- **Token Streaming**: Process outputs as they're generated\n",
    "- **Websocket Integration**: Real-time communication in web applications\n",
    "- **Incremental Processing**: Handle partial results effectively\n",
    "\n",
    "These features make LangChain particularly powerful for building sophisticated AI applications that go beyond simple prompt-response interactions, enabling complex workflows that combine language models with external data and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simple LLM call using LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: LangChain is a decentralized platform that aims to provide language learning opportunities for users through a combination of artificial intelligence, blockchain technology, and community-based learning. Users can access language courses, practice speaking with native speakers, and earn rewards for their participation in the platform. The use of blockchain technology ensures transparency and security of user data and transactions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the LLM (Make sure to set OPENAI_API_KEY in your environment)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7 , api_key=\"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\")\n",
    "\n",
    "# Example of a simple LLM call\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(\"LLM Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for Langchain components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't worry if you don't understand this right now. We'll explore all of this in detail in later notebooks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_2816\\303820560.py:36: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  agent_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "C:\\Users\\Abcom\\AppData\\Local\\Temp\\ipykernel_2816\\303820560.py:70: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qa_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response: AI stands for Artificial Intelligence, which refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding.\n",
      "QA Response: {'answer': 'Artificial Intelligence (AI) has transformed various industries, from healthcare to finance. Companies are leveraging AI to automate processes, improve decision-making, and enhance customer experiences. One of the most prominent applications of AI is in Natural Language Processing (NLP). Technologies like chatbots, virtual assistants, and AI-powered content generation have made communication more efficient. Moreover, AI in data analytics helps businesses extract valuable insights from large datasets, enabling smarter strategies and more informed decisions. As AI continues to evolve, ethical considerations, data privacy, and bias mitigation remain critical areas of focus.', 'confidence': 0.95}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Updated OpenAI imports\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers.structured import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# 1. Language Model (LLM)\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 2. Prompt Templates\n",
    "# Agent prompt template (requires agent_scratchpad)\n",
    "agent_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # For memory\n",
    "    (\"user\", \"{input}\"),  # User input\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")  # Required for OpenAI tools agent\n",
    "])\n",
    "\n",
    "# QA prompt template (doesn't need agent_scratchpad)\n",
    "qa_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # For memory\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "    (\"user\", \"Question: {input}\\nContext: {context}\")  # User input with context\n",
    "])\n",
    "\n",
    "# 3. Memory\n",
    "# For the agent - uses default input_key=\"input\"\n",
    "agent_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# For the QA chain - specify the input_key explicitly\n",
    "qa_memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\", return_messages=True)\n",
    "\n",
    "# 4. Agents and Tools\n",
    "def custom_tool_function(input_text):\n",
    "    return f\"You provided: {input_text}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"CustomTool\",\n",
    "        func=custom_tool_function,\n",
    "        description=\"A tool that echoes back the input text.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the OpenAI tools agent\n",
    "agent_executor = create_openai_tools_agent(llm, tools, agent_prompt_template)\n",
    "agent = AgentExecutor(agent=agent_executor, tools=tools, memory=agent_memory)\n",
    "\n",
    "# 5. Document Loaders\n",
    "loader = TextLoader(\"sample.txt\")  # Create a file named \"sample.txt\" with some text content.\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 6. Retrieval & Vector Stores\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 7. Create a QA chain with the correct prompt template\n",
    "qa_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=qa_prompt_template,  # Using the QA-specific prompt template\n",
    "    memory=qa_memory\n",
    ")\n",
    "\n",
    "# 8. Output Parsers\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", type=\"string\", description=\"The answer to the question\"),\n",
    "    ResponseSchema(name=\"confidence\", type=\"float\", description=\"Confidence score between 0 and 1\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Combine everything\n",
    "def process_query(question):\n",
    "    # Use the agent for dynamic decision-making\n",
    "    agent_response = agent.invoke({\"input\": question})\n",
    "    \n",
    "    # Retrieve relevant context from the vector store\n",
    "    relevant_context = vectorstore.similarity_search(question, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_context])\n",
    "    \n",
    "    # Use the QA chain for knowledge-based responses with instructions\n",
    "    formatted_question = f\"{question}\\n\\n{format_instructions}\"\n",
    "    qa_response = qa_chain.invoke({\n",
    "        \"input\": formatted_question, \n",
    "        \"context\": context\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Parse the output into structured data\n",
    "        parsed_output = output_parser.parse(qa_response[\"text\"])\n",
    "    except Exception as e:\n",
    "        # Handle parsing errors gracefully\n",
    "        parsed_output = {\"answer\": qa_response[\"text\"], \"confidence\": 0.5}\n",
    "        print(f\"Warning: Could not parse response - {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        \"agent_response\": agent_response[\"output\"],\n",
    "        \"qa_response\": parsed_output\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is AI\"\n",
    "    result = process_query(question)\n",
    "    print(\"Agent Response:\", result[\"agent_response\"])\n",
    "    print(\"QA Response:\", result[\"qa_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an AI-powered question-answering system using LangChain and OpenAI. Here's what it does step by step:\n",
    "\n",
    "1. **Setup**: It imports necessary libraries and sets up an OpenAI API key to access AI models.\n",
    "\n",
    "2. **Two AI Components**:\n",
    "   - An \"Agent\" that can use tools and make decisions\n",
    "   - A \"QA Chain\" that answers questions using relevant information\n",
    "\n",
    "3. **Memory System**: Both components remember previous conversations so they can maintain context.\n",
    "\n",
    "4. **Document Processing**:\n",
    "   - The system loads text from a file called \"sample.txt\"\n",
    "   - It breaks this text into smaller chunks\n",
    "   - It creates a searchable database of these chunks using embeddings (vector representations of text)\n",
    "\n",
    "5. **Question Processing Flow**:\n",
    "   - When you ask a question, the Agent tries to answer it using its tools\n",
    "   - The system also searches for relevant information in the document database\n",
    "   - The QA Chain uses this relevant information to provide a more informed answer\n",
    "   - The system tries to structure the answer with a confidence score\n",
    "\n",
    "6. **Output Format**: For each question, you get:\n",
    "   - An \"agent response\" (direct from the AI)\n",
    "   - A \"QA response\" (based on the document knowledge with structured format)\n",
    "\n",
    "The main advantage of this system is that it combines general AI capabilities with specific knowledge from your documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't worry if you don't understand this right now. We'll explore all of this in detail in later notebooks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Setup**\n",
    "\n",
    "Let's get started with installing LangChain and its dependencies. We'll go through this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Python (if not installed)\n",
    "Download and install Python (version 3.8 or later) from:\n",
    "🔗 https://www.python.org/downloads/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a Virtual Environment (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### On Windows (Command Prompt or PowerShell)\n",
    "- python -m venv langchain_env\n",
    "- langchain_env\\Scripts\\activate\n",
    "\n",
    "###### On macOS/Linux (Terminal)\n",
    "- python -m venv langchain_env\n",
    "- source langchain_env/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set Up OpenAI API Key\n",
    "You need an API key from OpenAI:\n",
    "\n",
    "- Sign up at 🔗 https://platform.openai.com/signup\n",
    "- Get your API key from 🔗 https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On Windows (Command Prompt)\n",
    "set OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "##### On PowerShell\n",
    "$env:OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run a Simple Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: There is a planet called HD 189733b where it rains glass sideways in 7,000 km/h winds.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Test call\n",
    "response = llm.invoke(\"Tell me a fun fact about space.\")\n",
    "\n",
    "print(\"LLM Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Now that we have covered the fundamentals and set up our environment, in the next notebooks we'll explore:\n",
    "1. Working with different types of LLMs\n",
    "2. Creating and using Chains\n",
    "3. Understanding Prompts and Templates\n",
    "4. Implementing Memory in our applications\n",
    "5. Working with Agents and Tools\n",
    "\n",
    "Each topic will include practical examples and real-world use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

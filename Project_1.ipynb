{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "RAG is a technique used in AI where a model retrieves relevant information from an external source (like a database or document storage) and then generates a response based on that information.\n",
    "\n",
    "## Simple Breakdown:\n",
    "1. **Retrieval** – The system searches for relevant documents or text from a knowledge base.  \n",
    "2. **Augmentation** – The retrieved information is given to the AI model as extra context.  \n",
    "3. **Generation** – The AI uses this information to generate a well-informed response.  \n",
    "\n",
    "## Why Use RAG?\n",
    "- **Improves accuracy** by providing up-to-date and relevant facts.  \n",
    "- **Reduces hallucination** (wrong or made-up answers).  \n",
    "- **Helps AI work with limited training data** by pulling real-time knowledge.  \n",
    "\n",
    "## Example:\n",
    "Imagine you have an AI chatbot for legal advice:  \n",
    "\n",
    "- The user asks: *\"What is the penalty for breaking a contract?\"*  \n",
    "- The AI **retrieves** relevant laws and case studies.  \n",
    "- It **augments** its knowledge with this data.  \n",
    "- It **generates** a response like:  \n",
    "\n",
    "  *\"According to XYZ law, breaking a contract may result in fines or legal action depending on the terms stated.\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key in the environment variable\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG-based Legal Document Analysis system**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base Query Engine\n",
    "- **Description:**\n",
    "\n",
    "Build a query engine that allows users to ask questions about a specific knowledge base (e.g., company policies or research papers etc). The engine retrieves relevant information and generates answers.\n",
    "\n",
    "### **LangChain Features Used:**\n",
    "\n",
    "- Document loaders to ingest and preprocess knowledge base data.\n",
    "- Retrieval-augmented generation (RAG) for answering questions.\n",
    "- Memory to refine search results based on user feedback.\n",
    "\n",
    "###  *Example Workflow:*\n",
    "- The user asks a question about the knowledge base.\n",
    "- The engine retrieves relevant documents and generates an answer.\n",
    "- The user can ask follow-up questions to refine the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export OPENAI_API_KEY=\"your_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE CONSTITUTION OF INDIA \n",
      "[As on       May, 2022] \n",
      "2022\n",
      "Loaded 1199 chunks of text.\n"
     ]
    }
   ],
   "source": [
    "# Ingest and Preprocess the Knowledge Base\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "# Load documents from the directory\n",
    "# loader = DirectoryLoader('knowledge_base/', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "# documents = loader.load()\n",
    "\n",
    "# Load documents from the PDF files\n",
    "documents = PyPDFLoader(r'C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf').load()\n",
    "print(documents[0].page_content)\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks of text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector Database\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate embeddings for the chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store embeddings in a FAISS vector database\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save the vector store locally (optional)\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Retrieval-Augmented Generation (RAG)\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Load the vector store\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Create the RAG chain\n",
    "\n",
    "\n",
    "# Add memory to the chain\n",
    "\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", output_key=\"result\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "\n",
    "\n",
    "qa_chain_with_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Knowledge Base Query Engine!\n",
      "\n",
      "Answer:\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "Source Documents:\n",
      "1. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "2. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "3. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "4. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "5. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "\n",
      "Answer:\n",
      "Hello! I can help answer questions, provide information, assist with problem-solving, and more. How can I assist you today?\n",
      "\n",
      "Source Documents:\n",
      "1. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "2. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "3. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "4. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "5. C:\\Users\\Abcom\\Desktop\\LangChain_Course\\Indian_Constitution.pdf\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def query_engine():\n",
    "    print(\"Welcome to the Knowledge Base Query Engine!\")\n",
    "    chat_history = []  # Store conversation history\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Retrieve and generate an answer\n",
    "        result = qa_chain_with_memory.invoke({\"question\": query, \"chat_history\": chat_history})  \n",
    "\n",
    "        answer = result[\"answer\"]  # Use \"answer\" instead of \"result\"\n",
    "        source_documents = result[\"source_documents\"]\n",
    "\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n",
    "\n",
    "        print(\"\\nSource Documents:\")\n",
    "        for i, doc in enumerate(source_documents):\n",
    "            print(f\"{i + 1}. {doc.metadata['source']}\")\n",
    "\n",
    "        # Update chat history\n",
    "        chat_history.append((query, answer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
